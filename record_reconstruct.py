"""
Record3D â€“ Enregistrement vidÃ©o + Reconstruction 3D volumÃ©trique (TSDF)
------------------------------------------------------------------------
Ce script enregistre un flux RGBD + poses camÃ©ra via Record3D,
puis reconstruit un environnement 3D via TSDF (Truncated Signed Distance
Function), le mÃªme principe que KinectFusion et l'app Record3D elle-mÃªme.

Pourquoi TSDF et pas accumulation de points ?
  - Une grille volumÃ©trique 3D est maintenue en mÃ©moire
  - Chaque frame RGBD met Ã  jour les voxels qu'elle voit (weighted average)
  - Les zones revisitÃ©es ne crÃ©ent PAS de doublons : les voxels existants
    sont juste re-moyennÃ©s â†’ fusion correcte par construction
  - Pas besoin d'ICP a posteriori

PrÃ©requis :
    pip install record3d open3d opencv-python numpy scipy

Usage :
    1. Branche l'iPhone en USB
    2. Record3D app â†’ Settings â†’ "USB Streaming mode" activÃ©
    3. Lance : python record_reconstruct.py
    4. Appuie sur âº dans l'app pour dÃ©marrer le flux

ContrÃ´les :
    [ESPACE]  â†’ dÃ©marrer / arrÃªter l'enregistrement
    [Q / ESC] â†’ quitter

Sorties (dans logs/<datetime>/) :
    - reconstructed.ply   : nuage de points avec normales
    - config.txt          : hyperparamÃ¨tres utilisÃ©s
    - performance.log     : mÃ©triques de performance
"""

import threading
import time
import os
from datetime import datetime
import numpy as np
import cv2
import open3d as o3d
from scipy.spatial.transform import Rotation
from record3d import Record3DStream


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  HYPERPARAMÃˆTRES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# â”€â”€ Mode de reconstruction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
USE_TSDF  = False    # True  â†’ TSDF volumÃ©trique (qualitÃ©, sans doublons)
                    # False â†’ accumulation rapide + voxel downsample (quasi temps-rÃ©el)
USE_GPU   = False   # True  â†’ TSDF GPU via Open3D tensor API (CUDA sur Jetson)
                    # False â†’ TSDF CPU (ScalableTSDFVolume)
                    # âš   USE_GPU=True nÃ©cessite open3d-cuda (pip install open3d-cuda)
                    #    et un device CUDA (Jetson, GPU NVIDIA) ou Metal (mac M-series)
                    #    Sur mac M4 : USE_GPU=False pour l'instant (pas de support CUDA)
                    #    Sur Jetson  : USE_GPU=True avec CUDA:0

# â”€â”€ Enregistrement â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
FRAME_SKIP        = 3       # Ne garder qu'1 frame sur N pendant l'enregistrement.
                            # 1 = toutes (lourd), 3 = bon compromis, 5+ = lÃ©ger
MIN_TRAVEL_DIST   = 0.01    # Distance min (m) entre 2 frames gardÃ©es.
                            # Ã‰vite de stocker quand la camÃ©ra est immobile.

# â”€â”€ Mode rapide (USE_TSDF=False) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SUBSAMPLE         = 2       # Sous-Ã©chantillonnage spatial du depth (1=dense, 2=moitiÃ©).
VOXEL_SIZE        = 0.01    # Taille du voxel de dÃ©duplication (m).
                            # 0.01 = 1cm absorbe le jitter ARKit (~2-5mm) â†’ pas de doublons.
                            # Plus petit = plus de doublons si drift ARKit > voxel.

# â”€â”€ TSDF (USE_TSDF=True) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TSDF_VOXEL_LENGTH = 0.005   # RÃ©solution de la grille TSDF (m). ParamÃ¨tre qualitÃ© principal.
                            # 0.003=3mm (dÃ©taillÃ©, lourd)  0.005=5mm (recommandÃ©)
                            # 0.008=8mm (rapide)  0.01=1cm (lÃ©ger)
TSDF_SDF_TRUNC    = 0.04    # Troncature SDF (m). RÃ¨gle : 4-8Ã— TSDF_VOXEL_LENGTH.
TSDF_BLOCK_COUNT  = 50000   # Nb blocs prÃ©-allouÃ©s (GPU uniquement).
                            # 50000 â‰ˆ piÃ¨ce standard. Augmenter pour grandes scÃ¨nes.

MAX_DEPTH         = 4.0     # Profondeur max (m). 2.0=bureau, 4.0=piÃ¨ce, 5+=extÃ©rieur.
CONFIDENCE_MIN    = 1       # Confidence LiDAR minimum (0=low, 1=medium, 2=high).

# â”€â”€ Filtrage post-reconstruction â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
OUTLIER_NB        = 20      # Nb voisins pour statistical outlier removal.
OUTLIER_STD       = 2.0     # Seuil en Ã©carts-types. Plus petit = plus agressif.

# â”€â”€ Normales â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
NORMAL_KNN        = 30      # Nb voisins pour estimation PCA des normales.
NORMAL_RADIUS     = 0.05    # Rayon max (m) pour chercher les voisins.

# â”€â”€ Preview â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
PREVIEW_W         = 1280    # Largeur max de la fenÃªtre preview OpenCV.


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  Collecte pour logs
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

HYPERPARAMS = {
    "USE_TSDF":          USE_TSDF,
    "USE_GPU":           USE_GPU,
    "FRAME_SKIP":        FRAME_SKIP,
    "MIN_TRAVEL_DIST":   MIN_TRAVEL_DIST,
    # Mode rapide
    "SUBSAMPLE":         SUBSAMPLE,
    "VOXEL_SIZE":        VOXEL_SIZE,
    # TSDF
    "TSDF_VOXEL_LENGTH": TSDF_VOXEL_LENGTH,
    "TSDF_SDF_TRUNC":    TSDF_SDF_TRUNC,
    "TSDF_BLOCK_COUNT":  TSDF_BLOCK_COUNT,
    # Commun
    "MAX_DEPTH":         MAX_DEPTH,
    "CONFIDENCE_MIN":    CONFIDENCE_MIN,
    "OUTLIER_NB":        OUTLIER_NB,
    "OUTLIER_STD":       OUTLIER_STD,
    "NORMAL_KNN":        NORMAL_KNN,
    "NORMAL_RADIUS":     NORMAL_RADIUS,
}


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  UTILITAIRES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def pose_to_matrix(qx, qy, qz, qw, tx, ty, tz):
    """
    Convertit un quaternion + translation (pose ARKit) en matrice 4Ã—4 homogÃ¨ne.
    ReprÃ©sente la transformation camera â†’ world.
    """
    R = Rotation.from_quat([qx, qy, qz, qw]).as_matrix()
    T = np.eye(4)
    T[:3, :3] = R
    T[:3, 3] = [tx, ty, tz]
    return T


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
#  CLASSE PRINCIPALE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

class Record3DRecorder:

    def __init__(self):
        self.session        = None
        self.stream_stopped = threading.Event()
        self.new_frame_evt  = threading.Event()

        # DerniÃ¨re frame reÃ§ue (pour la preview)
        self.latest_rgb        = None
        self.latest_depth      = None
        self.latest_intrinsic  = None
        self.latest_confidence = None
        self.latest_pose       = None
        self._lock             = threading.Lock()

        # Ã‰tat d'enregistrement
        self.recording          = False
        self.recorded_frames    = []
        self.record_start_time  = None
        self.last_pose_position = None
        self.frame_counter      = 0

    # â”€â”€ Utilitaires â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def get_intrinsic_mat_from_coeffs(self, coeffs, depth_w, depth_h, rgb_w, rgb_h):
        """Coefficients Record3D (repÃ¨re RGB) â†’ matrice intrinsÃ¨que au repÃ¨re depth."""
        scale_x = depth_w / rgb_w
        scale_y = depth_h / rgb_h
        return np.array([
            [coeffs.fx * scale_x, 0,                   coeffs.tx * scale_x],
            [0,                   coeffs.fy * scale_y, coeffs.ty * scale_y],
            [0,                   0,                   1                   ]
        ])

    # â”€â”€ Callbacks Record3D â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def on_new_frame(self):
        try:
            rgb    = self.session.get_rgb_frame()
            depth  = self.session.get_depth_frame()
            dH, dW = depth.shape[:2]
            rH, rW = rgb.shape[:2]
            coeffs = self.session.get_intrinsic_mat()
            intrinsic = self.get_intrinsic_mat_from_coeffs(coeffs, dW, dH, rW, rH)
            pose   = self.session.get_camera_pose()

            confidence = None
            if hasattr(self.session, "get_confidence_frame"):
                try:
                    confidence = self.session.get_confidence_frame()
                except Exception:
                    pass

            with self._lock:
                self.latest_rgb        = rgb
                self.latest_depth      = depth
                self.latest_intrinsic  = intrinsic
                self.latest_confidence = confidence
                self.latest_pose       = pose

            # â”€â”€ Enregistrement â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if self.recording:
                self.frame_counter += 1

                # Frame skip
                if self.frame_counter % FRAME_SKIP != 0:
                    self.new_frame_evt.set()
                    return

                # Distance travel check (Ã©vite les doublons quand la camÃ©ra est immobile)
                current_pos = np.array([pose.tx, pose.ty, pose.tz])
                if self.last_pose_position is not None:
                    dist = np.linalg.norm(current_pos - self.last_pose_position)
                    if dist < MIN_TRAVEL_DIST:
                        self.new_frame_evt.set()
                        return

                self.last_pose_position = current_pos

                # Resize RGB vers la rÃ©solution depth (Ã©conomie mÃ©moire)
                rgb_resized = cv2.resize(rgb, (dW, dH), interpolation=cv2.INTER_LINEAR)

                self.recorded_frames.append({
                    'rgb':        rgb_resized.copy(),
                    'depth':      depth.copy(),
                    'intrinsic':  intrinsic.copy(),
                    'confidence': confidence.copy() if confidence is not None else None,
                    'pose': {
                        'qx': pose.qx, 'qy': pose.qy, 'qz': pose.qz, 'qw': pose.qw,
                        'tx': pose.tx, 'ty': pose.ty, 'tz': pose.tz,
                    },
                    'timestamp': time.time(),
                })

            self.new_frame_evt.set()

        except Exception as e:
            print(f"[on_new_frame] Erreur : {e}")

    def on_stream_stopped(self):
        print("Stream arrÃªtÃ©.")
        self.stream_stopped.set()
        self.new_frame_evt.set()

    # â”€â”€ Enregistrement â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def start_recording(self):
        self.recorded_frames    = []
        self.frame_counter      = 0
        self.last_pose_position = None
        self.record_start_time  = time.time()
        self.recording          = True
        print("ğŸ”´ Enregistrement dÃ©marrÃ© â€” bouge l'iPhone lentement.")

    def stop_recording(self):
        self.recording = False
        record_duration = time.time() - self.record_start_time
        n = len(self.recorded_frames)
        print(f"â¹  Enregistrement arrÃªtÃ© : {n} frames en {record_duration:.1f}s")
        return record_duration

    # â”€â”€ Reconstruction rapide (accumulation + voxel) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def reconstruct_fast(self, frames):
        """
        Mode rapide (USE_TSDF=False) : dÃ©projection vectorisÃ©e + voxel downsample.
        Pas de doublons si VOXEL_SIZE â‰¥ jitter ARKit (~5-10mm).
        Temps typique : <1s pour 30 frames.
        """
        print(f"\nâš¡ Reconstruction rapide de {len(frames)} frames...")
        print(f"   subsample={SUBSAMPLE}  voxel={VOXEL_SIZE*1000:.0f}mm  max_depth={MAX_DEPTH}m")
        t0 = time.time()

        all_points, all_colors, skipped = [], [], 0

        for frame in frames:
            depth      = frame['depth']
            rgb        = frame['rgb']
            K          = frame['intrinsic']
            confidence = frame['confidence']
            H, W = depth.shape

            ys = np.arange(0, H, SUBSAMPLE)
            xs = np.arange(0, W, SUBSAMPLE)
            xv, yv = np.meshgrid(xs, ys)
            xv, yv = xv.flatten(), yv.flatten()

            z    = depth[yv, xv]
            mask = (z > 0) & (z < MAX_DEPTH)
            if confidence is not None:
                mask &= (confidence[yv, xv] >= CONFIDENCE_MIN)

            xv, yv, z = xv[mask], yv[mask], z[mask]
            if len(z) == 0:
                skipped += 1
                continue

            fx, cx, fy, cy = K[0,0], K[0,2], K[1,1], K[1,2]
            # ARKit convention: X right, Y up, Z toward viewer
            # OpenCV/pinhole convention: X right, Y down, Z into scene
            # â†’ flip Y and Z so local_pts match ARKit camera frame
            local_pts = np.stack([(xv-cx)*z/fx, -(yv-cy)*z/fy, -z], axis=-1)

            p = frame['pose']
            T = pose_to_matrix(p['qx'], p['qy'], p['qz'], p['qw'],
                               p['tx'], p['ty'], p['tz'])
            world_pts = (T[:3,:3] @ local_pts.T).T + T[:3,3]

            all_points.append(world_pts)
            all_colors.append(rgb[yv, xv].astype(np.float32) / 255.0)

        stats = {
            'mode': 'fast',
            'n_frames_total':   len(frames),
            'n_frames_skipped': skipped,
            'n_frames_used':    len(frames) - skipped,
        }

        if not all_points:
            print("âŒ Aucun point valide.")
            stats.update({'n_raw_points': 0, 'n_final_points': 0, 't_reconstruct': 0})
            return None, stats

        all_points = np.vstack(all_points)
        all_colors = np.vstack(all_colors)
        n_raw = len(all_points)
        print(f"  Points bruts : {n_raw:,}")

        pcd = o3d.geometry.PointCloud()
        pcd.points = o3d.utility.Vector3dVector(all_points)
        pcd.colors = o3d.utility.Vector3dVector(all_colors)

        print(f"  Voxel downsample ({VOXEL_SIZE*1000:.0f}mm)...")
        pcd = pcd.voxel_down_sample(VOXEL_SIZE)
        n_voxel = len(pcd.points)
        print(f"  AprÃ¨s voxel : {n_voxel:,} points ({100*n_voxel/n_raw:.1f}%)")

        print(f"  Outlier removal...")
        pcd, _ = pcd.remove_statistical_outlier(OUTLIER_NB, OUTLIER_STD)
        n_final = len(pcd.points)
        print(f"  Final : {n_final:,} points")

        t_recon = time.time() - t0
        print(f"  âœ… TerminÃ© en {t_recon:.2f}s")
        stats.update({'n_raw_points': n_raw, 'n_voxel_points': n_voxel,
                      'n_final_points': n_final, 't_reconstruct': t_recon})
        return pcd, stats

    # â”€â”€ Reconstruction TSDF (CPU ou GPU) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def reconstruct(self, frames):
        """
        Reconstruit l'environnement 3D via TSDF volumÃ©trique (ScalableTSDFVolume).

        Principe (identique Ã  KinectFusion / Record3D) :
        - Un volume 3D scalable est initialisÃ© une seule fois
        - Chaque frame RGBD est intÃ©grÃ©e via la pose ARKit (camera â†’ world)
        - Les zones revisitÃ©es RE-MOYENNENT les voxels existants
          â†’ pas de doublon possible par construction, contrairement Ã 
          l'accumulation de points
        - Extraction finale du nuage de points depuis la grille SDF

        Retourne (pcd, stats_dict) ou (None, stats_dict).
        """
        device_label = "GPU" if USE_GPU else "CPU"
        print(f"\nğŸ”¨ Reconstruction TSDF {device_label} de {len(frames)} frames...")
        print(f"   voxel={TSDF_VOXEL_LENGTH*1000:.0f}mm  sdf_trunc={TSDF_SDF_TRUNC*100:.0f}cm  max_depth={MAX_DEPTH}m")
        t0 = time.time()

        skipped      = 0
        n_integrated = 0

        if USE_GPU:
            # â”€â”€ TSDF GPU : Open3D tensor API (VoxelBlockGrid) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # Fonctionne avec CUDA (Jetson) ou Metal (Mac M-series si build avec support)
            # pip install open3d-cuda  sur Jetson
            try:
                import open3d.core as o3c
                # DÃ©tecte automatiquement CUDA ou Metal
                if o3c.cuda.is_available():
                    device = o3c.Device("CUDA:0")
                    print("  Device : CUDA:0")
                else:
                    device = o3c.Device("CPU:0")
                    print("  âš   CUDA non disponible â†’ fallback CPU (installe open3d-cuda)")

                vbg = o3d.t.geometry.VoxelBlockGrid(
                    attr_names   = ('tsdf', 'weight', 'color'),
                    attr_dtypes  = (o3c.float32, o3c.float16, o3c.float16),
                    attr_channels= ((1), (1), (3)),
                    voxel_size   = TSDF_VOXEL_LENGTH,
                    block_resolution = 16,
                    block_count  = TSDF_BLOCK_COUNT,
                    device       = device,
                )

                for i, frame in enumerate(frames):
                    if (i + 1) % 50 == 0 or i == len(frames) - 1:
                        print(f"  Frame {i+1}/{len(frames)} | intÃ©grÃ©es : {n_integrated}")

                    depth         = frame['depth'].copy()
                    rgb           = frame['rgb']
                    K             = frame['intrinsic']
                    confidence    = frame['confidence']
                    H, W          = depth.shape

                    if confidence is not None:
                        depth[confidence < CONFIDENCE_MIN] = 0.0

                    fx, cx = K[0,0], K[0,2]
                    fy, cy = K[1,1], K[1,2]

                    depth_t = o3c.Tensor(depth.astype(np.float32), device=device)
                    rgb_t   = o3c.Tensor(rgb.astype(np.uint8),   device=device)
                    intrinsic_t = o3c.Tensor(
                        np.array([[fx,0,cx],[0,fy,cy],[0,0,1]], dtype=np.float64)
                    )

                    p = frame['pose']
                    T_cam_to_world = pose_to_matrix(p['qx'], p['qy'], p['qz'], p['qw'],
                                                    p['tx'], p['ty'], p['tz'])
                    T_world_to_cam = np.linalg.inv(T_cam_to_world)
                    extrinsic_t = o3c.Tensor(T_world_to_cam.astype(np.float64))

                    try:
                        frustum_block_coords = vbg.compute_unique_block_coordinates(
                            depth_t, intrinsic_t, extrinsic_t,
                            depth_scale=1.0, depth_max=MAX_DEPTH
                        )
                        vbg.integrate(
                            frustum_block_coords,
                            depth_t, rgb_t,
                            intrinsic_t, intrinsic_t, extrinsic_t,
                            depth_scale=1.0, depth_max=MAX_DEPTH
                        )
                        n_integrated += 1
                    except Exception as e:
                        skipped += 1
                        if skipped <= 3:
                            print(f"  [WARN] Frame {i} ignorÃ©e : {e}")

                print("  Extraction depuis VoxelBlockGrid...")
                pcd_t = vbg.extract_point_cloud()
                pcd   = pcd_t.to_legacy()

            except ImportError:
                print("  âŒ open3d.core non disponible. Installe open3d-cuda ou repasse USE_GPU=False.")
                return None, {'mode': 'tsdf_gpu', 'n_frames_total': len(frames),
                              'n_frames_skipped': 0, 'n_frames_used': 0,
                              'n_raw_points': 0, 'n_final_points': 0, 't_reconstruct': 0}

        else:
            # â”€â”€ TSDF CPU : ScalableTSDFVolume â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            volume = o3d.pipelines.integration.ScalableTSDFVolume(
                voxel_length=TSDF_VOXEL_LENGTH,
                sdf_trunc=TSDF_SDF_TRUNC,
                color_type=o3d.pipelines.integration.TSDFVolumeColorType.RGB8,
            )

            for i, frame in enumerate(frames):
                if (i + 1) % 50 == 0 or i == len(frames) - 1:
                    print(f"  Frame {i+1}/{len(frames)} | intÃ©grÃ©es : {n_integrated}")

                depth         = frame['depth'].copy()
                rgb           = frame['rgb']
                K             = frame['intrinsic']
                confidence    = frame['confidence']
                H, W          = depth.shape

                if confidence is not None:
                    depth[confidence < CONFIDENCE_MIN] = 0.0

                fx, cx = K[0,0], K[0,2]
                fy, cy = K[1,1], K[1,2]
                intr  = o3d.camera.PinholeCameraIntrinsic(W, H, fx, fy, cx, cy)
                rgbd  = o3d.geometry.RGBDImage.create_from_color_and_depth(
                    o3d.geometry.Image(np.ascontiguousarray(rgb.astype(np.uint8))),
                    o3d.geometry.Image(np.ascontiguousarray(depth.astype(np.float32))),
                    depth_scale=1.0, depth_trunc=MAX_DEPTH,
                    convert_rgb_to_intensity=False,
                )

                p = frame['pose']
                T_cam_to_world = pose_to_matrix(p['qx'], p['qy'], p['qz'], p['qw'],
                                                p['tx'], p['ty'], p['tz'])
                # Open3D TSDF expects extrinsic in OpenCV convention (Yâ†“, Z forward).
                # ARKit pose is in ARKit convention (Yâ†‘, Z toward viewer).
                # R_fix converts from OpenCV cam to ARKit cam (flip Y and Z).
                R_fix = np.diag([1.0, -1.0, -1.0, 1.0])
                T_world_to_cam = np.linalg.inv(T_cam_to_world @ R_fix)

                try:
                    volume.integrate(rgbd, intr, T_world_to_cam)
                    n_integrated += 1
                except Exception as e:
                    skipped += 1
                    if skipped <= 3:
                        print(f"  [WARN] Frame {i} ignorÃ©e : {e}")

            print("  Extraction depuis ScalableTSDFVolume...")
            pcd = volume.extract_point_cloud()

        # â”€â”€ Stats + post-processing communs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        stats = {
            'mode':             f'tsdf_{"gpu" if USE_GPU else "cpu"}',
            'n_frames_total':   len(frames),
            'n_frames_skipped': skipped,
            'n_frames_used':    n_integrated,
        }

        if pcd is None or len(pcd.points) == 0:
            print("âŒ Volume TSDF vide â€” vÃ©rifie les poses ARKit et la depth.")
            stats.update({'n_raw_points': 0, 'n_final_points': 0, 't_reconstruct': 0})
            return None, stats

        n_raw = len(pcd.points)
        print(f"  Points extraits : {n_raw:,}")

        print(f"  Outlier removal (nb={OUTLIER_NB}, std={OUTLIER_STD})...")
        pcd, _ = pcd.remove_statistical_outlier(OUTLIER_NB, OUTLIER_STD)
        n_final = len(pcd.points)
        print(f"  Final : {n_final:,} points")

        t_recon = time.time() - t0
        print(f"  âœ… Reconstruction terminÃ©e en {t_recon:.2f}s")
        stats.update({'n_raw_points': n_raw, 'n_final_points': n_final, 't_reconstruct': t_recon})
        return pcd, stats

    # â”€â”€ Calcul des normales â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def compute_normals(self, pcd):
        """Estime les normales par PCA puis oriente vers l'origine."""
        print(f"  Calcul normales (knn={NORMAL_KNN}, radius={NORMAL_RADIUS})...")
        t0 = time.time()
        pcd.estimate_normals(
            search_param=o3d.geometry.KDTreeSearchParamHybrid(
                radius=NORMAL_RADIUS,
                max_nn=NORMAL_KNN
            )
        )
        pcd.orient_normals_towards_camera_location(np.array([0.0, 0.0, 0.0]))
        t_normals = time.time() - t0
        print(f"  âœ… Normales calculÃ©es en {t_normals:.2f}s")
        return pcd, t_normals

    # â”€â”€ Sauvegarde des logs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def save_logs(self, log_dir, record_duration, stats, t_normals):
        """Ã‰crit config.txt et performance.log dans log_dir."""
        os.makedirs(log_dir, exist_ok=True)

        # config.txt â€” hyperparamÃ¨tres
        with open(os.path.join(log_dir, "config.txt"), "w") as f:
            f.write("# HyperparamÃ¨tres de la reconstruction\n")
            f.write(f"# GÃ©nÃ©rÃ© le {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            for k, v in HYPERPARAMS.items():
                f.write(f"{k} = {v}\n")

        # performance.log â€” mÃ©triques
        t_recon  = stats.get('t_reconstruct', 0)
        t_total  = t_recon + t_normals
        with open(os.path.join(log_dir, "performance.log"), "w") as f:
            f.write(f"{'='*50}\n")
            f.write(f"  PERFORMANCE LOG\n")
            f.write(f"{'='*50}\n\n")
            f.write(f"Date                    : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            f.write(f"--- Enregistrement ---\n")
            f.write(f"DurÃ©e enregistrement    : {record_duration:.2f}s\n")
            f.write(f"Frames totales stockÃ©es : {stats.get('n_frames_total', 0)}\n")
            f.write(f"Frames utilisÃ©es        : {stats.get('n_frames_used', 0)}\n")
            f.write(f"Frames ignorÃ©es (erreur): {stats.get('n_frames_skipped', 0)}\n\n")
            mode = stats.get('mode', 'fast')
            if 'tsdf' in mode:
                f.write(f"--- Reconstruction TSDF ({mode}) ---\n")
                f.write(f"Voxel length            : {TSDF_VOXEL_LENGTH*1000:.0f}mm\n")
                f.write(f"SDF trunc               : {TSDF_SDF_TRUNC*100:.0f}cm\n")
                f.write(f"Points extraits (bruts) : {stats.get('n_raw_points', 0):,}\n")
                f.write(f"Points finaux           : {stats.get('n_final_points', 0):,}\n")
            else:
                f.write(f"--- Reconstruction Rapide (accumulation) ---\n")
                f.write(f"Subsample               : {SUBSAMPLE}\n")
                f.write(f"Voxel size              : {VOXEL_SIZE*1000:.0f}mm\n")
                f.write(f"Points bruts            : {stats.get('n_raw_points', 0):,}\n")
                f.write(f"Points aprÃ¨s voxel      : {stats.get('n_voxel_points', 0):,}\n")
                f.write(f"Points finaux           : {stats.get('n_final_points', 0):,}\n")
            f.write(f"Temps reconstruction    : {t_recon:.2f}s\n\n")
            f.write(f"--- Normales ---\n")
            f.write(f"Temps normales          : {t_normals:.2f}s\n\n")
            f.write(f"--- Total ---\n")
            f.write(f"Temps total pipeline    : {t_total:.2f}s\n")

        print(f"ğŸ“ Logs sauvegardÃ©s â†’ {log_dir}/")

    # â”€â”€ Pipeline complet post-enregistrement â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def process_recording(self, record_duration):
        """
        ExÃ©cute le pipeline complet :
        reconstruction â†’ normales â†’ sauvegarde PLY + logs.
        """
        if len(self.recorded_frames) < 2:
            print("âš ï¸  Pas assez de frames enregistrÃ©es (min 2).")
            return

        # 1. Reconstruction (dispatch selon USE_TSDF)
        if USE_TSDF:
            pcd, stats = self.reconstruct(self.recorded_frames)
        else:
            pcd, stats = self.reconstruct_fast(self.recorded_frames)
        if pcd is None:
            return

        # 2. Normales
        pcd, t_normals = self.compute_normals(pcd)

        # 3. Sauvegarde
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        log_dir   = os.path.join("logs", timestamp)
        os.makedirs(log_dir, exist_ok=True)

        ply_path = os.path.join(log_dir, "reconstructed.ply")
        o3d.io.write_point_cloud(ply_path, pcd)
        n_final = stats.get('n_final_points', len(pcd.points))
        print(f"ğŸ’¾ Nuage sauvegardÃ© â†’ {ply_path}  ({n_final:,} points + normales)")

        # 4. Logs
        self.save_logs(log_dir, record_duration, stats, t_normals)

        # LibÃ©rer la mÃ©moire des frames enregistrÃ©es
        self.recorded_frames = []

        print(f"\nâœ… Pipeline terminÃ©. Voir : {log_dir}/")
        print(f"   â†’ Visualiser : python view_ply.py {ply_path}")

    # â”€â”€ Boucle principale â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def run(self):
        devices = Record3DStream.get_connected_devices()
        if not devices:
            print("âŒ Aucun iPhone dÃ©tectÃ© via USB.")
            print("   â†’ VÃ©rifie que l'app Record3D est ouverte et que le streaming USB est activÃ©.")
            return

        print(f"âœ… Connexion Ã  : product_id={devices[0].product_id}")
        self.session = Record3DStream()
        self.session.on_new_frame      = self.on_new_frame
        self.session.on_stream_stopped = self.on_stream_stopped
        self.session.connect(devices[0])

        print("ğŸ¬ Flux prÃªt. Appuie sur âº dans Record3D pour dÃ©marrer.")
        print("   [ESPACE] â†’ dÃ©marrer/arrÃªter l'enregistrement   [Q/ESC] â†’ quitter")

        while not self.stream_stopped.is_set():
            self.new_frame_evt.wait(timeout=0.05)
            self.new_frame_evt.clear()

            with self._lock:
                rgb = self.latest_rgb

            if rgb is None:
                continue

            # â”€â”€ Preview 2D â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            bgr = cv2.cvtColor(rgb, cv2.COLOR_RGB2BGR)
            h, w = bgr.shape[:2]
            if w > PREVIEW_W:
                bgr = cv2.resize(bgr, (PREVIEW_W, int(PREVIEW_W * h / w)))

            if self.recording:
                n       = len(self.recorded_frames)
                elapsed = time.time() - self.record_start_time
                # Indicateur rouge clignotant
                if int(elapsed * 2) % 2 == 0:
                    cv2.circle(bgr, (30, 25), 10, (0, 0, 255), -1)
                cv2.putText(bgr, f"REC  {elapsed:.1f}s  |  {n} frames",
                            (50, 33), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
                cv2.putText(bgr, "[ESPACE] Arreter l'enregistrement",
                            (10, 65), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 200, 255), 2)
            else:
                cv2.putText(bgr, "[ESPACE] Enregistrer   [Q/ESC] Quitter",
                            (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 100), 2)

            cv2.imshow("Record3D â€“ Enregistrement", bgr)
            key = cv2.waitKey(1) & 0xFF

            if key in (ord('q'), 27):
                if self.recording:
                    record_duration = self.stop_recording()
                    cv2.destroyAllWindows()
                    self.process_recording(record_duration)
                print("Sortie demandÃ©e.")
                break

            if key == 32:  # ESPACE
                if not self.recording:
                    self.start_recording()
                else:
                    record_duration = self.stop_recording()
                    cv2.destroyAllWindows()
                    self.process_recording(record_duration)

                    if not self.stream_stopped.is_set():
                        print("\n   [ESPACE] â†’ nouvel enregistrement   [Q/ESC] â†’ quitter")

        cv2.destroyAllWindows()
        print("ğŸ‘‹ Programme terminÃ©.")


# â”€â”€ EntrÃ©e â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if __name__ == "__main__":
    recorder = Record3DRecorder()
    recorder.run()
